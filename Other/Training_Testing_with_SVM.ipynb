{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example From Pre-processing to Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute engine used:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "### General Packages ###\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import altair as alt\n",
    "\n",
    "### For Model Exporting ###\n",
    "from joblib import dump, load\n",
    "\n",
    "### Metrics for Evaluation ###\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "\n",
    "### For Board Modelling ###\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "### Ticket triage functions ###\n",
    "import sys\n",
    "sys.path.append(\"src/Auxiliary/\")\n",
    "sys.path.append(\"src/Cleaning/\")\n",
    "sys.path.append(\"src/Model/\")\n",
    "sys.path.append(\"src/Tokenizer/\")\n",
    "\n",
    "### For pre-processing ###\n",
    "import ticket_cleaner\n",
    "import bert_tokenizer\n",
    "\n",
    "### For Board Modelling ###\n",
    "import board\n",
    "\n",
    "### For Severity Modelling ###\n",
    "import severity\n",
    "\n",
    "### For Impact Modelling ###\n",
    "import impact\n",
    "\n",
    "### For Prs, Modified Accuracy Score ###\n",
    "import model_functions\n",
    "import numpy as np\n",
    "\n",
    "triage_metric = make_scorer(model_functions.modified_accuracy_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_excel(\"./Data/Tickets with Classifications.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ticket_cleaner.clean_tickets(ticketNbr = train_set.ticketNbr, contact_name = train_set.contact_name, company_name = train_set.company_name, Summary = train_set.Summary, Initial_Description = train_set.Initial_Description, Impact = train_set.SR_Impact_RecID, Severity = train_set.SR_Severity_RecID, Board = train_set.SR_Board_RecID, Source = train_set.Source, date_entered = train_set.date_entered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Board Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time (mins): 0:01:35\n",
      "Total Time (mins): 0:00:24\n"
     ]
    }
   ],
   "source": [
    "# Replace this with roBERTa\n",
    "#Imports\n",
    "from bert_tokenizer import BERT_Tokenizer\n",
    "import transformers as ppb\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "#Loading pre-trained models\n",
    "max_length = 100\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "#Tokenizers\n",
    "train, test = train_test_split(data,\n",
    "                   shuffle = True,\n",
    "                   train_size = 0.8,\n",
    "                   random_state = 1)\n",
    "\n",
    "X_text_train = BERT_Tokenizer(model = model, tokenizer = tokenizer, text = train.combined_text, max_len = max_length)\n",
    "X_text_test = BERT_Tokenizer(model = model, tokenizer = tokenizer, text = test.combined_text, max_len = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      " [[344   0   1]\n",
      " [  1 607  33]\n",
      " [  9  30 572]]\n",
      "Testing: \n",
      " [[ 78   4   6]\n",
      " [  5 139  28]\n",
      " [  8  15 117]]\n"
     ]
    }
   ],
   "source": [
    "#Model Board and Predict\n",
    "Y_train_board = train.Board\n",
    "Y_test_board = test.Board\n",
    "\n",
    "board_svm = SVC(C=1, kernel='linear', class_weight='balanced', gamma=0.1)\n",
    "board_svm.fit(X_text_train,Y_train_board)\n",
    "\n",
    "board_train = board_svm.predict(X_text_train)\n",
    "board_test = board_svm.predict(X_text_test)\n",
    "\n",
    "print(\"Training: \\n\",confusion_matrix(Y_train_board,board_train))\n",
    "print(\"Testing: \\n\",confusion_matrix(Y_test_board,board_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity and Impact Share these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Typically you would pass Raw text dataframe in but since we are using BERT + SVM as a placehold - we can skip re-tokenizing\n",
    "X_text_train = pd.DataFrame(X_text_train).assign(brd36 = [0]*len(X_text_train),brd41 = [0]*len(X_text_train),brd43 = [0]*len(X_text_train))\n",
    "X_text_test = pd.DataFrame(X_text_test).assign(brd36 = [0]*len(X_text_test),brd41 = [0]*len(X_text_test),brd43 = [0]*len(X_text_test))\n",
    "\n",
    "\n",
    "# Combine Text and OHE Source with Board Predictions \n",
    "X_features_train = severity.add_board_predictions(X_text_train, board_predict=board_train)\n",
    "X_features_test = severity.add_board_predictions(X_text_test, board_predict=board_test)\n",
    "\n",
    "# Encode Text with BERT\n",
    "### Normally would need to do this but same reason as above \n",
    "#X_text_src_board_train = severity.format_inputs(X_features_train, max_len = 100) \n",
    "#X_text_src_board_test = severity.format_inputs(X_features_test, max_len = 100) \n",
    "\n",
    "X_text_src_board_train = X_features_train\n",
    "X_text_src_board_test = X_features_test\n",
    "\n",
    "X_text_src_board_test = X_text_src_board_test.set_index(test.index)\n",
    "X_text_src_board_train = X_text_src_board_train.set_index(train.index)\n",
    "\n",
    "source_board = [\"email_connector\",\"deskdirector\",\"email\",\"renewal\",\"escalation\"]\n",
    "for i in source_board:\n",
    "    X_text_src_board_test[i] = test[i]\n",
    "    X_text_src_board_train[i] = train[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y Labels\n",
    "Y_severity_train = train.Severity\n",
    "Y_severity_test = test.Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Fitting Model--\n",
      "--Done--\n",
      "--Training--\n",
      "[[647 125  96]\n",
      " [ 69 346 128]\n",
      " [ 10  21 155]]\n",
      "0.8772698810269255\n",
      "--Testing\n",
      "[[154  45  25]\n",
      " [ 20  63  46]\n",
      " [  5  13  29]]\n",
      "0.8425\n"
     ]
    }
   ],
   "source": [
    "#Train Model\n",
    "severity_svm_model, severity_train = severity.train_svm(X_text_src_board_train,Y_severity_train, model_name = \"model1\", save_model = \"N\", export_path = \"./\", verbose=2)\n",
    "\n",
    "#Use Model to Predict (This is the command tyyo load from joblib file)\n",
    "#pred_probs = severity.predict_svm(X_predict = X_text_src_board_test, import_path = \"./model1.joblib\", verbose = 2)\n",
    "\n",
    "#For the Dashboard\n",
    "predictions_severity = severity_svm_model.predict(X_text_src_board_test)\n",
    "\n",
    "print(\"--Training--\")\n",
    "print(confusion_matrix(Y_severity_train,severity_train.Predict))\n",
    "print(model_functions.modified_accuracy_score(y_true = Y_severity_train, y_predict = severity_train.Predict))\n",
    "\n",
    "print(\"--Testing\")\n",
    "print(confusion_matrix(Y_severity_test,predictions_severity))\n",
    "print(model_functions.modified_accuracy_score(y_true = Y_severity_test, y_predict = predictions_severity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Y Labels\n",
    "Y_impact_train = train.Impact\n",
    "Y_impact_test = test.Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Fitting Model--\n",
      "--Done--\n",
      "--Training--\n",
      "[[905 173 101]\n",
      " [ 26 195  44]\n",
      " [ 11  21 121]]\n",
      "0.9004383218534753\n",
      "--Testing\n",
      "[[227  43  27]\n",
      " [ 13  41  11]\n",
      " [  9  10  19]]\n",
      "0.8525\n"
     ]
    }
   ],
   "source": [
    "#Train Model\n",
    "impact_svm_model, impact_train = impact.train_svm(X_text_src_board_train,Y_impact_train, model_name = \"model1\", save_model = \"N\", export_path = \"./\", verbose=2)\n",
    "\n",
    "#Use Model to Predict (This is the command tyyo load from joblib file)\n",
    "#pred_probs = impact.predict_svm(X_predict = X_text_src_board_test, import_path = \"./model1.joblib\", verbose = 2)\n",
    "\n",
    "#For the Dashboard\n",
    "predictions_impact = impact_svm_model.predict(X_text_src_board_test)\n",
    "\n",
    "print(\"--Training--\")\n",
    "print(confusion_matrix(Y_impact_train,impact_train.Predict))\n",
    "print(model_functions.modified_accuracy_score(y_true = Y_impact_train, y_predict = impact_train.Predict))\n",
    "\n",
    "print(\"--Testing\")\n",
    "print(confusion_matrix(Y_impact_test,predictions_impact))\n",
    "print(model_functions.modified_accuracy_score(y_true = Y_impact_test, y_predict = predictions_impact))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[:,[\"Impact\",\"Severity\",\"Board\"]].assign(Board_Predictions = board_train, Severity_Predictions = severity_train.Predict, Impact_Predictions = impact_train.Predict)\r\n",
    "train[\"Subset\"] = \"Train\"\r\n",
    "test = test.loc[:,[\"Impact\",\"Severity\",\"Board\"]].assign(Board_Predictions = board_test, Severity_Predictions = predictions_severity, Impact_Predictions = predictions_impact)\r\n",
    "test[\"Subset\"] = \"Test\"\r\n",
    "output_df = train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Impact</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Board</th>\n",
       "      <th>Board_Predictions</th>\n",
       "      <th>Severity_Predictions</th>\n",
       "      <th>Impact_Predictions</th>\n",
       "      <th>Subset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1597 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Impact  Severity  Board  Board_Predictions  Severity_Predictions  \\\n",
       "0          0         2     41                 41                     2   \n",
       "813        0         0     43                 43                     0   \n",
       "1152       0         0     43                 43                     0   \n",
       "361        0         0     36                 36                     0   \n",
       "1726       1         1     41                 41                     2   \n",
       "...      ...       ...    ...                ...                   ...   \n",
       "1791       1         2     41                 41                     1   \n",
       "1096       1         1     41                 41                     2   \n",
       "1932       0         1     41                 41                     2   \n",
       "235        0         2     41                 41                     2   \n",
       "1061       0         2     43                 43                     0   \n",
       "\n",
       "      Impact_Predictions Subset  \n",
       "0                      0  Train  \n",
       "813                    0  Train  \n",
       "1152                   0  Train  \n",
       "361                    0  Train  \n",
       "1726                   1  Train  \n",
       "...                  ...    ...  \n",
       "1791                   2  Train  \n",
       "1096                   1  Train  \n",
       "1932                   1  Train  \n",
       "235                    0  Train  \n",
       "1061                   1  Train  \n",
       "\n",
       "[1597 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_custom_metric = np.mean(cross_val_score(impact_svm_model, X_text_src_board_test, Y_impact_test, scoring=triage_metric, cv=10))\n",
    "impact_f1_micro = np.mean(cross_val_score(impact_svm_model, X_text_src_board_test, Y_impact_test, scoring=\"f1_micro\", cv=10))\n",
    "impact_f1_weighted = np.mean(cross_val_score(impact_svm_model, X_text_src_board_test, Y_impact_test, scoring=\"f1_weighted\", cv=10))\n",
    "\n",
    "severity_custom_metric = np.mean(cross_val_score(severity_svm_model, X_text_src_board_test, Y_severity_test, scoring=triage_metric, cv=10))\n",
    "severity_f1_micro = np.mean(cross_val_score(severity_svm_model, X_text_src_board_test, Y_severity_test, scoring=\"f1_micro\", cv=10))\n",
    "severity_f1_weighted = np.mean(cross_val_score(severity_svm_model, X_text_src_board_test, Y_severity_test, scoring=\"f1_weighted\", cv=10))\n",
    "\n",
    "board_accuracy = accuracy_score(Y_test_board, board_test)\n",
    "board_f1_score = f1_score(Y_test_board, board_test, average = \"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [impact_custom_metric\n",
    ",impact_f1_micro\n",
    ",impact_f1_weighted\n",
    ",severity_custom_metric\n",
    ",severity_f1_micro\n",
    ",severity_f1_weighted\n",
    ",board_accuracy\n",
    ",board_f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impact_custom_metric</th>\n",
       "      <th>impact_f1_micro</th>\n",
       "      <th>impact_f1_weighted</th>\n",
       "      <th>severity_custom_metric</th>\n",
       "      <th>severity_f1_micro</th>\n",
       "      <th>severity_f1_weighted</th>\n",
       "      <th>board_accuracy</th>\n",
       "      <th>board_f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.686454</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>0.5775</td>\n",
       "      <td>0.590862</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.835418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impact_custom_metric  impact_f1_micro  impact_f1_weighted  \\\n",
       "0                  0.81           0.6525            0.686454   \n",
       "\n",
       "   severity_custom_metric  severity_f1_micro  severity_f1_weighted  \\\n",
       "0                  0.8175             0.5775              0.590862   \n",
       "\n",
       "   board_accuracy  board_f1_score  \n",
       "0           0.835        0.835418  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_metrics = pd.DataFrame(columns = [\"impact_custom_metric\",\"impact_f1_micro\",\"impact_f1_weighted\",\"severity_custom_metric\",\"severity_f1_micro\",\"severity_f1_weighted\",\"board_accuracy\",\"board_f1_score\"])\n",
    "output_metrics.loc[0] = [impact_custom_metric ,impact_f1_micro ,impact_f1_weighted ,severity_custom_metric ,severity_f1_micro ,severity_f1_weighted ,board_accuracy ,board_f1_score]\n",
    "output_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8100000000000002,\n",
       " 0.6525000000000001,\n",
       " 0.6864540441924161,\n",
       " 0.8175000000000001,\n",
       " 0.5775,\n",
       " 0.590862493756491,\n",
       " 0.835,\n",
       " 0.8354175667869154]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(output_metrics.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
